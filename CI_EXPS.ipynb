{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOpwqCyRJTN2P7hEvRkAReO"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ptnKa4O7LUqc",
        "outputId": "f67485db-1b33-4595-ecc7-b3fc0179718e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[-0.50043927  0.67897416 -0.61625306  0.40761954 -0.17873758]\n",
            " [-0.50043927  0.67897416 -0.65472966  0.40028483 -0.82335911]\n",
            " [-0.50043927  0.67897416 -0.69320626  0.1834901  -0.82335911]\n",
            " [-0.50043927  0.67897416 -0.57777646 -0.06949721 -0.82335911]\n",
            " [-0.50043927  0.67897416 -0.50082325 -0.46340711 -0.17873758]]\n",
            "[0 0 0 0 0]\n"
          ]
        }
      ],
      "source": [
        "# Main Block For EXP 2 3 5\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load the dataset (replace with your actual file path)\n",
        "data = pd.read_csv('SolarPrediction.csv')\n",
        "\n",
        "# Drop unnecessary columns like Time, UNIXTime (if not required for prediction)\n",
        "data = data.drop(columns=['Time', 'TimeSunRise', 'TimeSunSet', 'UNIXTime', 'Data'])\n",
        "\n",
        "# Check for missing values and drop or fill them\n",
        "data = data.dropna()  # Drop rows with missing values\n",
        "\n",
        "# Split data into features (X) and target (y)\n",
        "X = data.drop(columns=['Radiation'])  # Assuming Radiation is the target\n",
        "y = data['Radiation']\n",
        "\n",
        "# Convert 'Radiation' into binary (0 or 1), threshold at 0.5\n",
        "threshold = 300\n",
        "y = np.where(y >= threshold, 1, 0)\n",
        "\n",
        "# Normalize the features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "print(X_scaled[:5])\n",
        "print(y[:5])\n",
        "\n",
        "# Split data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# EXP 5 single\n",
        "class SingleLayerPerceptron:\n",
        "    def __init__(self, learning_rate=0.01, n_iter=1000):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.n_iter = n_iter\n",
        "        self.weights = None\n",
        "        self.bias = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        n_samples, n_features = X.shape\n",
        "        # Initialize weights and bias\n",
        "        self.weights = np.zeros(n_features)\n",
        "        self.bias = 0\n",
        "\n",
        "        # Training the model\n",
        "        for iteration in range(self.n_iter):\n",
        "            for idx, x_i in enumerate(X):\n",
        "                linear_output = np.dot(x_i, self.weights) + self.bias\n",
        "                y_predicted = self.activation_function(linear_output)\n",
        "\n",
        "                # Update weights and bias\n",
        "                update = self.learning_rate * (y[idx] - y_predicted)\n",
        "                self.weights += update * x_i\n",
        "                self.bias += update\n",
        "\n",
        "            # Debug: Print loss every 100 iterations\n",
        "            if iteration % 100 == 0:\n",
        "                loss = np.mean((y - self.predict(X)) ** 2)  # Mean Squared Error\n",
        "                print(f\"Iteration {iteration}, Loss: {loss:.4f}\")\n",
        "\n",
        "    def activation_function(self, x):\n",
        "        return np.where(x >= 0, 1, 0)\n",
        "\n",
        "    def predict(self, X):\n",
        "        linear_output = np.dot(X, self.weights) + self.bias\n",
        "        return self.activation_function(linear_output)\n",
        "\n",
        "# Instantiate and train SLP\n",
        "slp = SingleLayerPerceptron(learning_rate=0.01, n_iter=1000)\n",
        "slp.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "slp_predictions = slp.predict(X_test)\n",
        "slp_accuracy = np.mean(slp_predictions == y_test)\n",
        "print(f'Single Layer Perceptron Accuracy: {slp_accuracy:.2f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "59_lw8EhRbnU",
        "outputId": "27153c17-9e78-43f7-af49-5b6decccc819"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 0, Loss: 0.1677\n",
            "Iteration 100, Loss: 0.1637\n",
            "Iteration 200, Loss: 0.1524\n",
            "Iteration 300, Loss: 0.1859\n",
            "Iteration 400, Loss: 0.1637\n",
            "Iteration 500, Loss: 0.1617\n",
            "Iteration 600, Loss: 0.1399\n",
            "Iteration 700, Loss: 0.1768\n",
            "Iteration 800, Loss: 0.1553\n",
            "Iteration 900, Loss: 0.1338\n",
            "Single Layer Perceptron Accuracy: 0.86\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# EXP 5 Multi\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Create MLP model for regression\n",
        "mlp = MLPRegressor(hidden_layer_sizes=(5,), max_iter=1000, random_state=42)\n",
        "\n",
        "# Fit MLP model\n",
        "mlp.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "mlp_predictions = mlp.predict(X_test)\n",
        "\n",
        "# Calculate Mean Squared Error (MSE) for evaluation\n",
        "mse = mean_squared_error(y_test, mlp_predictions)\n",
        "print(f'Multi-Layer Perceptron MSE: {mse:.2f}')\n",
        "\n",
        "# Optionally, you can calculate R^2 score to evaluate performance\n",
        "r2_score = mlp.score(X_test, y_test)\n",
        "print(f'Multi-Layer Perceptron R^2 Score: {r2_score:.2f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZxG03MH5UEP0",
        "outputId": "e80b88f9-47e6-443c-95c4-6dcef7afbc4e"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Multi-Layer Perceptron MSE: 0.09\n",
            "Multi-Layer Perceptron R^2 Score: 0.55\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# EXP 2\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Initialize the Logistic Regression model\n",
        "model = LogisticRegression()\n",
        "\n",
        "# Fit the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate Mean Squared Error for regression evaluation\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f'Mean Squared Error: {mse}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "42CUvAYSLdFw",
        "outputId": "b9b3c86d-2746-4b99-d1dc-96af0869c8b5"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error: 0.1258794738452126\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install deap"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ONz8Of5rOtfB",
        "outputId": "312c0054-506f-43ce-e9a4-69e328d88342"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting deap\n",
            "  Downloading deap-1.4.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from deap) (1.26.4)\n",
            "Downloading deap-1.4.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (135 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.4/135.4 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: deap\n",
            "Successfully installed deap-1.4.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# EXP 3\n",
        "from deap import base, creator, tools, algorithms\n",
        "import random\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "# Create the fitness function (minimizing MSE)\n",
        "creator.create(\"FitnessMin\", base.Fitness, weights=(-1.0,))\n",
        "creator.create(\"Individual\", list, fitness=creator.FitnessMin)\n",
        "\n",
        "# Define the hyperparameter space\n",
        "def create_individual():\n",
        "    # Generate C as a positive float in the range (0.001, 100)\n",
        "    C = random.uniform(0.001, 100)\n",
        "    return [C]\n",
        "\n",
        "# Fitness function to evaluate the individual\n",
        "def evaluate(individual):\n",
        "    C = individual[0]\n",
        "\n",
        "    # Create and train the model with the current hyperparameter\n",
        "    model = LogisticRegression(C=C, random_state=42)\n",
        "    try:\n",
        "        scores = cross_val_score(model, X_train, y_train, cv=5, scoring='neg_mean_squared_error')\n",
        "        return (np.mean(-scores),)  # Return positive MSE\n",
        "    except Exception as e:\n",
        "        return (float('inf'),)  # Return a very high MSE if there's an error\n",
        "\n",
        "# Initialize DEAP components\n",
        "toolbox = base.Toolbox()\n",
        "toolbox.register(\"individual\", tools.initIterate, creator.Individual, create_individual)\n",
        "toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n",
        "toolbox.register(\"evaluate\", evaluate)\n",
        "toolbox.register(\"mate\", tools.cxBlend, alpha=0.5)\n",
        "toolbox.register(\"mutate\", tools.mutGaussian, mu=0, sigma=0.1, indpb=0.2)\n",
        "toolbox.register(\"select\", tools.selTournament, tournsize=3)\n",
        "\n",
        "# Ensure that mutations do not generate out-of-bound values\n",
        "def mutate(individual):\n",
        "    for i in range(len(individual)):\n",
        "        if random.random() < 0.2:  # Mutation probability\n",
        "            # Mutate C\n",
        "            new_value = individual[i] + random.gauss(0, 0.1)  # Gaussian mutation\n",
        "            # Clip to ensure the new value is within bounds\n",
        "            individual[i] = max(0.001, min(100, new_value))\n",
        "    return individual,\n",
        "\n",
        "toolbox.register(\"mutate\", mutate)\n",
        "\n",
        "# Run the Genetic Algorithm\n",
        "population = toolbox.population(n=10)\n",
        "ngen = 10  # Number of generations\n",
        "cxpb = 0.5  # Crossover probability\n",
        "mutpb = 0.2  # Mutation probability\n",
        "\n",
        "# Run the Genetic Algorithm\n",
        "algorithms.eaSimple(population, toolbox, cxpb, mutpb, ngen, verbose=True)\n",
        "\n",
        "# Extract the best solution\n",
        "best_ind = tools.selBest(population, k=1)[0]\n",
        "print(f'Best Regularization Strength: {best_ind[0]}')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9QzAxQnBNDFK",
        "outputId": "baf6cb13-fd87-4b63-c7ed-7a5c4ac9f085"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/deap/creator.py:185: RuntimeWarning: A class named 'FitnessMin' has already been created and it will be overwritten. Consider deleting previous creation of that class or rename it.\n",
            "  warnings.warn(\"A class named '{0}' has already been created and it \"\n",
            "/usr/local/lib/python3.10/dist-packages/deap/creator.py:185: RuntimeWarning: A class named 'Individual' has already been created and it will be overwritten. Consider deleting previous creation of that class or rename it.\n",
            "  warnings.warn(\"A class named '{0}' has already been created and it \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gen\tnevals\n",
            "0  \t10    \n",
            "1  \t8     \n",
            "2  \t6     \n",
            "3  \t6     \n",
            "4  \t8     \n",
            "5  \t7     \n",
            "6  \t2     \n",
            "7  \t8     \n",
            "8  \t5     \n",
            "9  \t7     \n",
            "10 \t6     \n",
            "Best Regularization Strength: 3.0769056629418183\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# EXP 4\n",
        "import random\n",
        "import numpy as np\n",
        "# Objective function\n",
        "def objective_function(x):\n",
        " return x[0]**2 + x[1]**2 + 1\n",
        "# PSO parameters\n",
        "num_dimensions = 2\n",
        "num_particles = 30\n",
        "max_iterations = 100\n",
        "# Initialize particles\n",
        "particles = np.random.rand(num_particles, num_dimensions)\n",
        "velocities = np.random.rand(num_particles, num_dimensions)\n",
        "personal_best_positions = particles.copy()\n",
        "personal_best_values = np.array([objective_function(x) for x in particles])\n",
        "# Initialize global best\n",
        "global_best_position = personal_best_positions[np.argmin(personal_best_values)]\n",
        "global_best_value = min(personal_best_values)\n",
        "# PSO loop\n",
        "for iteration in range(max_iterations):\n",
        " for i in range(num_particles):\n",
        " # Update velocities\n",
        " velocities[i] = velocities[i] + \\\n",
        " random.random() * (personal_best_positions[i] - particles[i]) + \\\n",
        " random.random() * (global_best_position - particles[i])\n",
        "\n",
        " # Update particle positions\n",
        " particles[i] += velocities[i]\n",
        "\n",
        " # Update personal bests\n",
        " current_value = objective_function(particles[i])\n",
        " if current_value < personal_best_values[i]:\n",
        " personal_best_positions[i] = particles[i]\n",
        " personal_best_values[i] = current_value\n",
        "\n",
        " # Update global best\n",
        " if current_value < global_best_value:\n",
        " global_best_position = particles[i]\n",
        " global_best_value = current_value\n",
        "print(f\"Best position: {global_best_position}\")\n",
        "print(f\"Best value: {global_best_value}\")\n"
      ],
      "metadata": {
        "id": "Cbvyg6SxOjn5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "j0g4BSnsRaET"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}